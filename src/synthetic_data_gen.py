"""
M√≥dulo para gera√ß√£o de dados sint√©ticos usando Ollama.
"""
import os
import json
import ollama
from pypdf import PdfReader
from tqdm import tqdm
from dotenv import load_dotenv

# ==========================================
# CONFIGURA√á√ïES
# ==========================================

# Carrega vari√°veis de ambiente
load_dotenv()

# Onde est√£o seus PDFs/TXTs originais
SOURCE_DIR = os.getenv("SYNTHETIC_SOURCE_DIR", "data/source_documents")

# Onde o arquivo pronto para treino ser√° salvo
OUTPUT_FILE = os.getenv("SYNTHETIC_OUTPUT_FILE",
                        "data/raw/train_data_synthetic.jsonl")

# Modelo que vai GERAR os dados (deve estar rodando no Ollama)
GENERATOR_MODEL = os.getenv("SYNTHETIC_GENERATOR_MODEL", "llama3.1")

# A Instru√ß√£o do Sistema (Persona) que ser√° gravada no dataset
# A Instru√ß√£o do Sistema (Persona) que ser√° gravada no dataset
SYSTEM_INSTRUCTION = os.getenv("SYNTHETIC_SYSTEM_INSTRUCTION", "")

if not SYSTEM_INSTRUCTION:
    print("‚ö†Ô∏è AVISO: Vari√°vel SYNTHETIC_SYSTEM_INSTRUCTION n√£o definida!")
    print("   Usando fallback vazio (Isso pode prejudicar o treino).")


# ==========================================
# FUN√á√ïES
# ==========================================


def _read_file_content(filepath):
    """
    L√™ o conte√∫do de um arquivo PDF ou TXT.
    """
    content = ""
    if filepath.endswith(".pdf"):
        reader = PdfReader(filepath)
        for page in reader.pages:
            extracted = page.extract_text()
            if extracted:
                content += extracted + "\n"
    elif filepath.endswith(".txt"):
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()
    return content


def _create_chunks(content, filename):
    """
    Divide o conte√∫do em chunks para processamento.
    """
    chunks = []
    # Chunk de ~2000 caracteres (aprox 500 tokens)
    chunk_size = 2000
    # Sobreposi√ß√£o para n√£o perder contexto
    overlap = 200

    for i in range(0, len(content), chunk_size - overlap):
        chunk = content[i:i+chunk_size]
        if len(chunk) > 100:  # Ignora peda√ßos muito pequenos
            chunks.append({
                "source": filename,
                "text": chunk
            })
    return chunks


def extract_text_from_files(directory):
    """
    L√™ todos os arquivos .pdf e .txt da pasta especificada.
    Retorna uma lista de dicion√°rios com 'source' (nome do arquivo) e 'text'
    (conte√∫do).
    """
    documents = []

    # Verifica se a pasta existe
    if not os.path.exists(directory):
        print(f"‚ùå Erro: A pasta '{directory}' n√£o existe.")
        return []

    files = [f for f in os.listdir(directory) if f.endswith(('.pdf', '.txt'))]
    print(f"üìÇ Encontrados {len(files)} arquivos em: {directory}")

    for filename in files:
        filepath = os.path.join(directory, filename)

        try:
            content = _read_file_content(filepath)

            # Se extraiu texto, quebra em peda√ßos (chunks) para n√£o estourar o
            # limite do Ollama
            if content:
                documents.extend(_create_chunks(content, filename))

        except Exception as e:  # pylint: disable=broad-exception-caught
            print(f"‚ö†Ô∏è Erro ao ler {filename}: {e}")
            continue

    print(f"‚úÖ Texto extra√≠do e fragmentado em {len(documents)} partes.")
    return documents


def generate_synthetic_data(documents):
    """
    Usa o Ollama para ler cada trecho de texto e criar pares
    de Pergunta/Resposta.
    """
    print(
        f"ü§ñ Iniciando gera√ß√£o com o modelo '{GENERATOR_MODEL}'... "
        "(Pode demorar)"
    )

    generated_rows = []

    # Barra de progresso para acompanhar
    for doc in tqdm(documents, desc="Processando documentos"):

        # O Prompt que pede para o LLM criar os dados
        prompt = (
            f"Analise o seguinte texto t√©cnico extra√≠do do arquivo "
            f"'{doc['source']}':\n\n"
            f"TEXTO:\n"
            f"\"{doc['text']}\"\n\n"
            f"TAREFA:\n"
            f"Atue como um especialista em criar datasets para treinamento.\n"
            f"Crie 2 pares de intera√ß√£o Usu√°rio/Assistente baseados "
            f"EXCLUSIVAMENTE neste texto.\n"
            f"As perguntas devem simular um usu√°rio do sistema ERP Planuze "
            f"com d√∫vidas reais.\n\n"
            f"Retorne APENAS um JSON v√°lido (lista de objetos) no seguinte "
            f"formato, sem markdown:\n\n"
            f"[\n"
            f"    {{\n"
            f"        \"contexto\": \"Resumo curto e denso da informa√ß√£o\",\n"
            f"        \"pergunta\": \"Pergunta natural do usu√°rio\",\n"
            f"        \"resposta\": \"Resposta t√©cnica baseada no texto\"\n"
            f"    }}\n"
            f"]"
        )

        try:
            # Chamada ao Ollama
            response = ollama.chat(model=GENERATOR_MODEL, messages=[
                {
                    'role': 'system',
                    'content': ('Voc√™ √© um gerador de datasets JSON estrito. '
                                'Responda apenas com JSON v√°lido.')
                },
                {'role': 'user', 'content': prompt},
            ])

            content = response['message']['content']

            # Limpeza cir√∫rgica do JSON
            content = content.replace("```json", "").replace("```", "").strip()

            # Tenta converter string para JSON
            data = json.loads(content)

            # Formata para o padr√£o final do treino (Unsloth/Alpaca format)
            for item in data:
                row = {
                    "instruction": SYSTEM_INSTRUCTION,
                    # O Input simula o que o sistema RAG entregaria para o
                    # modelo em produ√ß√£o
                    "input": (f"[TEMA]: Documenta√ß√£o {doc['source']}\n"
                              f"[CONTEXTO]: {item['contexto']}\n"
                              f"[PERGUNTA]: {item['pergunta']}"),
                    "output": item['resposta']
                }
                generated_rows.append(row)

        except json.JSONDecodeError:
            # Erro comum: O modelo falou algo antes do JSON ou errou a v√≠rgula.
            # Ignoramos este chunk.
            continue
        except Exception as e:  # pylint: disable=broad-exception-caught
            print(f"\n‚ùå Erro na API Ollama: {e}")
            continue

    return generated_rows


def save_jsonl(data, filename):
    """Salva a lista de objetos em um arquivo .jsonl"""
    # Garante que a pasta de destino existe (data/raw)
    os.makedirs(os.path.dirname(filename), exist_ok=True)

    print(f"üíæ Salvando {len(data)} exemplos em {filename}...")

    try:
        with open(filename, 'w', encoding='utf-8') as f:
            for entry in data:
                json.dump(entry, f, ensure_ascii=False)
                f.write('\n')
        print("üöÄ Sucesso! Arquivo gerado.")

    except Exception as e:  # pylint: disable=broad-exception-caught
        print(f"‚ùå Erro ao salvar arquivo: {e}")


# ==========================================
# EXECU√á√ÉO PRINCIPAL
# ==========================================
if __name__ == "__main__":
    print("--- INICIANDO GERADOR SINT√âTICO PLANUS ---")

    # 1. Extrair Texto
    docs = extract_text_from_files(SOURCE_DIR)

    if docs:
        # 2. Gerar Dados com IA
        dataset = generate_synthetic_data(docs)

        if dataset:
            # 3. Salvar Resultado
            save_jsonl(dataset, OUTPUT_FILE)
        else:
            print(
                "‚ö†Ô∏è O modelo n√£o gerou dados v√°lidos. "
                "Verifique se Ollama est√° rodando."
            )
    else:
        print("‚ö†Ô∏è Nenhum documento encontrado.")
        print(f"üëâ Coloque arquivos .pdf ou .txt na pasta: {SOURCE_DIR}")
