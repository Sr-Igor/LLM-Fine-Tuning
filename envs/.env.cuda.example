# ==========================================
# üèãÔ∏è TRAINING CONFIGURATIONS (CUDA/Unsloth)
# ==========================================
# Rename to envs/.env.cuda
# ==========================================

# Base Model for Fine-Tuning
# Suggestions (Unsloth supports all major architectures):
# --- General Purpose (All-rounders) ---
# - unsloth/Meta-Llama-3.1-8B-Instruct   (The Standard)
# - unsloth/Meta-Llama-3.1-70B-Instruct  (Powerful, needs 48GB+ VRAM)
# - unsloth/Mistral-NeMo-12B-Instruct-v1 (New architecture, very efficient)
#
# --- Coding & Reasoning (Best for Logic) ---
# - unsloth/Qwen2.5-32B-Instruct         (Excellent reasoning/coding balance)
# - unsloth/DeepSeek-R1-Distill-Llama-8B (Great reasoning capability)
# - unsloth/Yi-Coder-9B-Chat             (Specialized for code)
#
# --- Small/Edge (Fast training, low VRAM) ---
# - unsloth/gemma-2-9b-it                (Google's SOTA for size)
# - unsloth/Phi-3.5-mini-instruct        (Microsoft, very fast)
# - unsloth/Qwen2.5-7B-Instruct          (Best 7B class)
MODEL_NAME="unsloth/Qwen2.5-32B-Instruct"

# Maximum Sequence Length (Context Window)
MAX_SEQ_LENGTH=2048

# Load in 4-bit to save memory? (True/False)
LOAD_IN_4BIT=true

# --- Training Parameters ---

# How many training steps to execute.
TRAINING_MAX_STEPS=60

# Batch Size (Examples processed simultaneously per GPU).
TRAINING_BATCH_SIZE=2

# Gradient Accumulation.
TRAINING_GRAD_ACCUMULATION=4

# Warmup Steps.
TRAINING_WARMUP_STEPS=5

# Learning Rate.
TRAINING_LEARNING_RATE=2e-4

# Random seed for reproducibility.
TRAINING_SEED=3407

# Number of processes (CPUs) to process the dataset.
TRAINING_DATASET_NUM_PROC=2

# Folder where intermediate checkpoints will be saved.
TRAINING_OUTPUT_DIR="outputs_checkpoints"

# Path to the FINAL processed Dataset to be used in training
DATASET_PATH="data/processed/train_dataset_final.jsonl"

# Where to save the final converted model (GGUF)
FINAL_MODEL_NAME="models/llm_qwen_v1"

# ==========================================
# ‚öôÔ∏è ADVANCED TRAINING PARAMETERS
# ==========================================

# Optimizer. "adamw_8bit"
TRAINING_OPTIM="adamw_8bit"

# Weight Decay (L2 Regularization).
TRAINING_WEIGHT_DECAY=0.01

# LR Scheduler Type. "linear" or "cosine"
TRAINING_LR_SCHEDULER="linear"

# ==========================================
# üîß LORA & PEFT CONFIG (Efficient Fine-Tuning)
# ==========================================

LORA_R=16
LORA_ALPHA=16
LORA_DROPOUT=0
LORA_TARGET_MODULES='["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]'
LORA_RANDOM_STATE=3407

# Quantization method for GGUF export.
GGUF_QUANTIZATION="q4_k_m"

# ==========================================
# ‚öôÔ∏è SYSTEM AND CACHE (Advanced)
# ==========================================

# HF_HOME=/mnt/data/huggingface_cache
CUDA_VISIBLE_DEVICES=0
