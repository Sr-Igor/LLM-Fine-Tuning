# ==========================================
# üîê AUTHENTICATION AND ACCESS
# ==========================================

# Hugging Face Token (REQUIRED)
# Required to download "Gated" models like Llama 3.1 or to upload your final model.
# Create a token with "Write" permission at: https://huggingface.co/settings/tokens
HF_TOKEN=hf_your_token_here_xyz123

# Repository for Model Upload (Optional)
# Ex: your-username/your-model
HF_REPO_ID=

# ==========================================
# üìä MONITORING (Optional, but Recommended)
# ==========================================

# Weights & Biases (WandB)
# Generates beautiful charts for Loss, VRAM, and real-time progress in the browser.
# Create an account at: https://wandb.ai/
WANDB_API_KEY=wandb_your_api_key_here
WANDB_PROJECT="<your_project_name>"
WANDB_WATCH=false  # false is lighter, true logs gradients (heavy)

# ==========================================
# ü§ñ OLLAMA CONFIGURATION (Data Generator)
# ==========================================

# Ollama Address (Default: http://localhost:11434)
# If running on docker or another machine, change it here.
OLLAMA_HOST=http://localhost:11434

# Model used to read your PDFs and create questions/answers.
# Recommended Options (Ollama):
# - qwen2.5:14b       (Excellent logic, good balance)
# - llama3.1:8b       (Solid general purpose)
# - deepseek-r1:7b    (New specialized reasoning model)
# - qwen2.5-coder:7b  (If your data is technical/code related)
# - phi3.5            (High speed processing)
GENERATOR_MODEL=qwen2.5:14b

# ==========================================
# üß© SYNTHETIC GENERATOR (Global)
# ==========================================

# Batch 1: Small chunks, PT only
# export SYNTHETIC_CHUNK_SIZE=2500
# export SYNTHETIC_OVERLAP=300
# export SYNTHETIC_LANGUAGES="pt"

# Batch 2: Large chunks, multilingual
# export SYNTHETIC_CHUNK_SIZE=4500
# export SYNTHETIC_OVERLAP=700
# export SYNTHETIC_LANGUAGES="pt,en,es"

# Batch 3: Different model
# export SYNTHETIC_CHUNK_SIZE=3500
# export SYNTHETIC_OVERLAP=500
# export SYNTHETIC_GENERATOR_MODEL="llama3.1"
# export SYNTHETIC_LANGUAGES="pt,en"

# Chunk size for PDF reading (in characters)
SYNTHETIC_CHUNK_SIZE=3500

# Overlap between chunks to maintain context
SYNTHETIC_OVERLAP=500

# Directory containing the original PDFs
SYNTHETIC_SOURCE_DIR="data/source_documents"

# Path where the generated synthetic JSONL will be saved
SYNTHETIC_OUTPUT_FILE="data/raw/train_data_synthetic.jsonl"

# Model used to GENERATE data via Ollama
SYNTHETIC_GENERATOR_MODEL="qwen2.5:14b"

# Tasks configuration file (JSON)
SYNTHETIC_TASKS_FILE="data/config/synthetic_tasks.json"

# Supported languages for generation (comma separated)
SYNTHETIC_LANGUAGES="pt,en"

# System instruction (Persona) injected into each generated example
# SYNTHETIC_SYSTEM_INSTRUCTION="You are a RAG agent, an intelligent AI assistant integrated into the ERP"
AI_PROMPT_ASK_INSTRUCTIONS="You are Planus, a specialized AI assistant for the ERP."
AI_PROMPT_ACTION_INSTRUCTIONS="You are Planus, the Planuze ERP AI assistant with the capability to EXECUTE ACTIONS in the system."

# ==========================================
# ‚öôÔ∏è CONTEXT (Templates)
# ==========================================
AI_CHAT_SUBJECT=subject
AI_CHAT_CONTEXT=context
AI_CHAT_QUESTION=question
AI_CHAT_HISTORY=history
AI_CHAT_LANGUAGE=language
AI_CHAT_MODE=mode
AI_CHAT_SYSTEM_REQUIRED=system_action_required
AI_CHAT_SYSTEM_INSTRUCTIONS=system_instructions
