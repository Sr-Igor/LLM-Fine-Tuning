# @title üöÄ Pipeline Master: Protocolo Planus (Unsloth + GGUF + Hugging Face)
# @markdown Este script automatiza todo o processo documentado: Setup, Treino, Compila√ß√£o Segura e Upload.

from datasets import load_dataset
from transformers import TrainingArguments
from trl import SFTTrainer
from unsloth import FastLanguageModel
from huggingface_hub import login, HfApi
from dotenv import load_dotenv
import os
import sys
import torch
from google.colab import drive
import psutil

# ==========================================
# ‚öôÔ∏è ZONA DE CONFIGURA√á√ÉO (Edite aqui)
# ==========================================

# Nome da pasta raiz do projeto no seu Google Drive
# Exemplo: Se o zip foi extra√≠do em "MyDrive/llm/meu-projeto", coloque "meu-projeto"
PROJECT_ROOT_FOLDER = "planuze-llm-collab"

# Caminho relativo do dataset dentro do projeto
DATASET_RELATIVE_PATH = "data/processed/train_dataset_final.jsonl"

# Nome que voc√™ quer dar ao modelo no Hugging Face
MODEL_REPO_NAME = "planus-qwen-v1"

# Configura√ß√µes de Treino (Otimizadas para Tesla T4)
MAX_SEQ_LENGTH = 1024  # 1024 √© o limite seguro para T4. 2048 pode dar OOM.
LOAD_IN_4BIT = True

# ==========================================
# üõ†Ô∏è 1. PREPARA√á√ÉO DO AMBIENTE
# ==========================================
print("üèóÔ∏è [1/6] Preparando Ambiente e Montando Drive...")

# 1.1 Montar Drive
drive.mount('/content/drive', force_remount=True)

# 1.2 Definir Caminhos Din√¢micos
# Procura a pasta do projeto recursivamente para evitar erros de caminho
search_cmd = f"find /content/drive/MyDrive -type d -name '{PROJECT_ROOT_FOLDER}' -print -quit"
project_path_list = os.popen(search_cmd).read().strip()

if not project_path_list:
    raise FileNotFoundError(
        f"‚ùå A pasta '{PROJECT_ROOT_FOLDER}' n√£o foi encontrada no seu Drive via busca.")

PROJECT_PATH = project_path_list
print(f"‚úÖ Diret√≥rio do Projeto localizado: {PROJECT_PATH}")

# 1.3 Instalar Depend√™ncias (Silencioso)
print("üì¶ Instalando Unsloth e depend√™ncias (pode levar 2-3 min)...")
!pip install - -no-deps - q "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install - -no-deps - q "xformers<0.0.29" "trl<0.9.0" peft accelerate bitsandbytes python-dotenv huggingface_hub

# ==========================================
# üîê 2. AUTENTICA√á√ÉO E CONFIGURA√á√ÉO
# ==========================================
print("\nüîê [2/6] Configurando Autentica√ß√£o...")


# 2.1 Carregar .env
env_path = os.path.join(PROJECT_PATH, ".env")
if os.path.exists(env_path):
    load_dotenv(env_path)
    print("‚úÖ Arquivo .env carregado.")
else:
    print(
        f"‚ö†Ô∏è .env n√£o encontrado em {env_path}. Tentando vari√°veis de ambiente do sistema.")

# 2.2 Login no Hugging Face
hf_token = os.getenv("HF_TOKEN")
if not hf_token:
    raise ValueError(
        "‚ùå ERRO CR√çTICO: HF_TOKEN n√£o encontrado. Verifique seu .env.")

try:
    login(token=hf_token)
    api = HfApi()
    user_info = api.whoami()
    username = user_info['name']

    # Valida√ß√£o de Permiss√£o de Escrita
    if 'write' not in user_info['auth']['accessToken']['role'] and user_info['auth']['accessToken']['role'] != 'write':
        # Nota: A API as vezes retorna estruturas diferentes, mas tentamos validar.
        print("‚ö†Ô∏è AVISO: Verifique se seu token tem permiss√£o 'WRITE'. Tokens 'READ' falhar√£o no upload.")

    FULL_REPO_ID = f"{username}/{MODEL_REPO_NAME}"
    print(f"‚úÖ Autenticado como: {username}")
    print(f"üéØ Target Repo: {FULL_REPO_ID}")

except Exception as e:
    raise RuntimeError(f"‚ùå Falha na autentica√ß√£o com Hugging Face: {e}")

# ==========================================
# üß† 3. TREINAMENTO (UNSLOTH)
# ==========================================
print("\nüß† [3/6] Iniciando Rotina de Treino...")


# 3.1 Carregar Modelo Base
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2.5-7B-Instruct",
    max_seq_length=MAX_SEQ_LENGTH,
    dtype=None,
    load_in_4bit=LOAD_IN_4BIT,
)

# 3.2 Configurar LoRA
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj",
                    "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)

# 3.3 Carregar Dataset
dataset_full_path = os.path.join(PROJECT_PATH, DATASET_RELATIVE_PATH)
if not os.path.exists(dataset_full_path):
    raise FileNotFoundError(
        f"‚ùå Dataset n√£o encontrado em: {dataset_full_path}")

dataset = load_dataset("json", data_files=dataset_full_path, split="train")
print(f"üìö Dataset carregado: {len(dataset)} registros.")

# Formata√ß√£o do Prompt (Alpaca/ChatML Style)


def formatting_prompts_func(examples):
    # Adapte esta fun√ß√£o se seu JSONL tiver chaves diferentes de instruction/input/output
    if "messages" in examples:  # Suporte a formato chat direto
        return {"text": [tokenizer.apply_chat_template(m, tokenize=False) for m in examples['messages']]}

    # Fallback gen√©rico
    texts = []
    for instruction, input, output in zip(examples.get("instruction", []), examples.get("input", []), examples.get("output", [])):
        text = f"<|im_start|>system\n{instruction}<|im_end|>\n<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n{output}<|im_end|>"
        texts.append(text)
    return {"text": texts, }


# 3.4 Configurar Treinador
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=MAX_SEQ_LENGTH,
    dataset_num_proc=2,
    packing=False,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        max_steps=100,  # Ajuste conforme necess√°rio
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs",
    ),
)

# 3.5 Executar Treino
print("üî• Treinando...")
trainer_stats = trainer.train()
print("‚úÖ Treino conclu√≠do!")

# 3.6 Salvar Adaptadores Localmente (Backup no Drive)
backup_dir = os.path.join(PROJECT_PATH, "outputs_checkpoints", "final_adapter")
model.save_pretrained(backup_dir)
tokenizer.save_pretrained(backup_dir)
print(f"üíæ Backup dos adaptadores salvo em: {backup_dir}")

# ==========================================
# üî® 4. COMPILA√á√ÉO DO LLAMA.CPP (A PROVA DE FALHAS)
# ==========================================
print("\nüî® [4/6] Compilando llama.cpp (Modo Seguro -j 1)...")

# Script Shell embutido para garantir ambiente limpo
shell_script = """
cd /content
rm -rf llama.cpp
git clone --depth 1 https://github.com/ggerganov/llama.cpp
cd llama.cpp
mkdir build
cd build
cmake .. -DGGML_NATIVE=OFF
cmake --build . --config Release -j 1
"""
os.system(shell_script)

# Link Simb√≥lico Cr√≠tico
if os.path.exists("/content/llama.cpp/build/bin/llama-quantize"):
    os.system(
        "ln -sf /content/llama.cpp/build/bin/llama-quantize /content/llama.cpp/llama-quantize")
    print("‚úÖ Compila√ß√£o OK e Link Simb√≥lico criado.")
else:
    raise RuntimeError(
        "‚ùå Erro na compila√ß√£o do llama.cpp. Bin√°rio n√£o encontrado.")

# ==========================================
# ‚òÅÔ∏è 5. EXPORTA√á√ÉO E UPLOAD
# ==========================================
print("\n‚òÅÔ∏è [5/6] Iniciando Convers√£o GGUF e Upload...")

# Define o m√©todo de quantiza√ß√£o
quant_method = "q4_k_m"

print(f"üöÄ Enviando para Hugging Face: {FULL_REPO_ID}")
print("‚òï Isso pode levar alguns minutos (Convers√£o + Upload de ~5GB)...")

try:
    model.push_to_hub_gguf(
        FULL_REPO_ID,
        tokenizer,
        quantization_method=quant_method,
        token=hf_token
    )
    print("\nüéâ ===================================================")
    print(f"‚úÖ SUCESSO ABSOLUTO! O MODELO EST√Å ONLINE.")
    print(f"üîó Link: https://huggingface.co/{FULL_REPO_ID}")
    print("======================================================")

except Exception as e:
    print(f"\n‚ùå Erro no Upload Autom√°tico: {e}")
    print("üí° Tentativa de recupera√ß√£o: Verifique se o repo j√° existe ou tente upload manual do arquivo .gguf gerado.")

# ==========================================
# üèÅ 6. INSTRU√á√ïES FINAIS
# ==========================================
print("\nüèÅ [6/6] Pr√≥ximos Passos (No seu Mac):")
print(f"1. Instale o Ollama: brew install ollama")
print(f"2. Rode direto: ollama run hf.co/{FULL_REPO_ID}")
