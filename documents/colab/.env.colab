## ESSE √â UM ARQUIVO DE SUGEST√ÉO DE CONFIGURA√á√ÉO DAS ENVS PARA O COLAB FREE
## VALORES OTIMIZADOS PARA GPU TESLA T4 (16GB VRAM)

# ==========================================
# üîê AUTENTICA√á√ÉO E ACESSOS
# ==========================================
# Crie um token "Write" em: https://huggingface.co/settings/tokens
HF_TOKEN=hf_

# ==========================================
# üìä MONITORAMENTO (Opcional, mas Recomendado)
# ==========================================
WANDB_API_KEY=wandb_
WANDB_PROJECT=planus-llm
WANDB_WATCH=false

# ==========================================
# ü§ñ CONFIGURA√á√ïES DO OLLAMA (Gerador de Dados)
# ==========================================
OLLAMA_HOST=http://localhost:11434

# No Colab, usamos o 7B para gera√ß√£o se for rodar local na inst√¢ncia (CPU/T4)
GENERATOR_MODEL=qwen2.5:7b

# Par√¢metros de Gera√ß√£o Sint√©tica (Ajustados para Colab/T4)
SYNTHETIC_SOURCE_DIR="data/source_documents"
SYNTHETIC_OUTPUT_FILE="data/raw/train_data_synthetic.jsonl"
SYNTHETIC_GENERATOR_MODEL="qwen2.5:7b"
SYNTHETIC_CHUNK_SIZE=1500
SYNTHETIC_OVERLAP=150

# Instru√ß√£o Simplificada para economizar tokens/mem√≥ria na gera√ß√£o local do Colab
SYNTHETIC_SYSTEM_INSTRUCTION="Voc√™ √© o Assistente Planus. Responda tecnicamente sobre o ERP Planuze. Regras: 1. Use PT-BR. 2. Datas dd/mm/aaaa. 3. Nunca cite IDs internos ou SQL. 4. Seja direto."

# Configura√ß√µes dos prefixos de chat (Prompt Template)
AI_CHAT_SUBJECT=SUBJECT
AI_CHAT_CONTEXT=CONTEXT
AI_CHAT_QUESTION=QUESTION
AI_CHAT_HISTORY=HISTORY
AI_CHAT_LANGUAGE=LANGUAGE

# ==========================================
# üèãÔ∏è CONFIGURA√á√ïES DE TREINAMENTO (SFTTrainer)
# ==========================================

# Modelo Base: 7B √© o limite seguro para a Tesla T4 (16GB VRAM)
# Tentar 32B ou 14B causar√° OOM (Out of Memory)
MODEL_NAME="unsloth/Qwen2.5-7B-Instruct"

# Context Window (CR√çTICO)
# Reduzido para 1024 para garantir estabilidade na T4
MAX_SEQ_LENGTH=1024

# Obrigat√≥rio para GPUs de consumo/Colab
LOAD_IN_4BIT=true

# --- Par√¢metros de Treino ---
TRAINING_MAX_STEPS=100
TRAINING_BATCH_SIZE=2
TRAINING_GRAD_ACCUMULATION=4
TRAINING_WARMUP_STEPS=10
TRAINING_LEARNING_RATE=2e-4
TRAINING_SEED=3407
# Limitado a 2 n√∫cleos no Colab
TRAINING_DATASET_NUM_PROC=2

# --- Diret√≥rios ---
TRAINING_OUTPUT_DIR="outputs_checkpoints"
DATASET_PATH="data/processed/train_dataset_final.jsonl"
FINAL_MODEL_NAME="models/planus_qwen_v1_colab"

# ==========================================
# ‚öôÔ∏è PAR√ÇMETROS AVAN√áADOS DE TREINO
# ==========================================
TRAINING_OPTIM="adamw_8bit"
TRAINING_WEIGHT_DECAY=0.01
TRAINING_LR_SCHEDULER="linear"

# ==========================================
# üîß LORA & PEFT CONFIG (Ajuste Fino Eficiente)
# ==========================================
LORA_R=16
LORA_ALPHA=16
LORA_DROPOUT=0
LORA_TARGET_MODULES='["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]'
LORA_RANDOM_STATE=3407

# M√©todo de quantiza√ß√£o para exporta√ß√£o GGUF
GGUF_QUANTIZATION="q4_k_m"

# ==========================================
# ‚öôÔ∏è SISTEMA E CACHE (Avan√ßado)
# ==========================================
# HF_HOME=/content/cache (Opcional, Colab gerencia isso)
CUDA_VISIBLE_DEVICES=0