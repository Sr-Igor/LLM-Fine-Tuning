# üìò Protocolo Planus: Pipeline de Fine-Tuning e Exporta√ß√£o de LLMs

**Stack:** Unsloth (Qwen2.5), Google Colab (T4 GPU), Google Drive, Hugging Face, Ollama.
**Objetivo:** Treinar modelos adaptados ao contexto de neg√≥cio e export√°-los para execu√ß√£o local via GGUF.

---

## üèóÔ∏è Fase 1: Prepara√ß√£o do Ambiente (Zero Ground)

O sucesso da exporta√ß√£o depende de como os arquivos s√£o estruturados no in√≠cio.

### 1. Upload e Estrutura

1.  **Compacta√ß√£o:** No seu computador local, compacte (ZIP) a pasta do seu projeto contendo o dataset `data/processed` e os scripts necess√°rios.
2.  **Upload:** Suba o arquivo `.zip` para a **raiz** do seu Google Drive.
3.  **Descompacta√ß√£o Controlada:** No Colab, **n√£o** descompacte na raiz `/content` (que √© vol√°til) se quiser persist√™ncia, mas para performance de I/O, `/content` √© melhor.
    - _Recomenda√ß√£o:_ Mantenha o dataset e c√≥digos pesados no Drive para persist√™ncia, mas esteja ciente da lat√™ncia. Ou copie para `/content` no in√≠cio da sess√£o.

### 2. Mapeamento de Caminhos (Din√¢mico)

Como o nome da pasta pode mudar (ex: `meu-projeto-v1`, `planus-final`, `teste-dev`), **nunca** use caminhos absolutos hardcoded ("chumbados") nos scripts.

**Script de Setup Inicial no Colab:**

```python
import os
from google.colab import drive

# 1. Montar Drive
drive.mount('/content/drive')

# 2. Definir a Raiz do Projeto (VARI√ÅVEL CR√çTICA)
# Altere APENAS esta linha conforme o nome da pasta atual no seu Drive
PROJECT_ROOT_NAME = "planuze-llm-collab"

# Caminho absoluto constru√≠do dinamicamente
PROJECT_PATH = f"/content/drive/MyDrive/llm/{PROJECT_ROOT_NAME}"

print(f"üìÇ Diret√≥rio de trabalho definido: {PROJECT_PATH}")
```

---

## üß† Fase 2: Treinamento (Fine-Tuning)

Utilize o **Unsloth** para efici√™ncia de mem√≥ria e velocidade.

1.  **Depend√™ncias:** Instalar `unsloth[colab-new]`.
2.  **Configura√ß√£o:** Carregar modelo base (ex: `unsloth/Qwen2.5-7B-Instruct`) em 4-bit.
3.  **Treino:** Executar `SFTTrainer`.
4.  **Salvamento dos Adaptadores (Checkpoints):**
    - O Unsloth salva apenas os adaptadores (arquivos pequenos, ~200MB).
    - **Dica de Ouro:** Configure o `output_dir` para salvar os checkpoints dentro do Drive (`f"{PROJECT_PATH}/outputs"`) para n√£o perd√™-los se o Colab desconectar.

---

## ‚ö†Ô∏è Fase 3: O Gargalo da Exporta√ß√£o (Aprendizados Cr√≠ticos)

Esta √© a fase onde 90% dos erros ocorrem (Disco cheio, RAM estourada, Permiss√µes).

### Aprendizado 1: O Dilema do Disco

O Google Colab tem disco local limitado. Tentar fazer o merge do modelo full (15GB) + quantiza√ß√£o (5GB) no `/content` pode falhar por falta de espa√ßo.

- **Solu√ß√£o:** Usar o m√©todo `push_to_hub_gguf` direto (se suportado) ou montar diret√≥rios tempor√°rios no Drive.

### Aprendizado 2: Compila√ß√£o do llama.cpp

A ferramenta de convers√£o (GGUF) precisa ser compilada. O `make` padr√£o falha ou o processo √© morto pela gest√£o de mem√≥ria do Colab ("Killed").

- **Solu√ß√£o:** Usar `cmake` com limita√ß√£o de threads (`-j 1`) para poupar mem√≥ria.

**Script de Build "√Ä Prova de Falhas":**

```bash
%%bash
# Garante execu√ß√£o na raiz vol√°til do Colab (mais r√°pido que o Drive para compilar)
cd /content
git clone --depth 1 https://github.com/ggerganov/llama.cpp
cd llama.cpp
mkdir build && cd build

# -DGGML_NATIVE=OFF aumenta compatibilidade
# -j 1 evita o erro "Killed" (OOM) na Tesla T4
cmake .. -DGGML_NATIVE=OFF
cmake --build . --config Release -j 1
```

### Aprendizado 3: O Link Simb√≥lico

O Unsloth espera o bin√°rio `llama-quantize` na raiz da pasta `llama.cpp`, mas o CMake o cria dentro de `build/bin`.

**A√ß√£o Obrigat√≥ria:**

```python
!ln -sf /content/llama.cpp/build/bin/llama-quantize /content/llama.cpp/llama-quantize
```

---

## ‚òÅÔ∏è Fase 4: Autentica√ß√£o e Upload (Hugging Face)

**Erros comuns:** 401 (N√£o autorizado) e 403 (Proibido - Namespace errado).

### Checklist de Autentica√ß√£o:

1.  **Token:** Precisa ser do tipo **WRITE** (Escrita). Tokens de leitura geram erro 401 na cria√ß√£o do reposit√≥rio.
2.  **Arquivo .env:** Se estiver no Drive, o `load_dotenv()` precisa do caminho completo.

```python
from dotenv import load_dotenv
load_dotenv(f"{PROJECT_PATH}/.env") # Usa a vari√°vel din√¢mica da Fase 1
```

3.  **Namespace (O erro 403):** Voc√™ n√£o pode criar um repo para uma organiza√ß√£o que n√£o pertence (ex: `planuze/modelo`) se o seu usu√°rio for `joao-dev` e n√£o tiver permiss√£o.

**Script de Upload Seguro:**

```python
from huggingface_hub import HfApi, login
import os

# 1. Autentica√ß√£o
token = os.getenv("HF_TOKEN")
if not token:
    raise ValueError("HF_TOKEN n√£o encontrado!")
login(token=token)

# 2. Identifica√ß√£o Autom√°tica do Usu√°rio (Evita erro 403)
api = HfApi()
username = api.whoami()['name']
repo_name = "planus-qwen-v1" # Nome do modelo desejado
full_repo_id = f"{username}/{repo_name}"

print(f"üöÄ Enviando para: {full_repo_id}")

# 3. Upload (Exemplo de upload manual, caso o m√©todo autom√°tico falhe)
api.create_repo(repo_id=full_repo_id, repo_type="model", exist_ok=True)
api.upload_file(
    path_or_fileobj="qwen2.5-7b-instruct.Q4_K_M.gguf", # Arquivo local gerado
    path_in_repo="planus.gguf",
    repo_id=full_repo_id
)
```

---

## üñ•Ô∏è Fase 5: Consumo Local (Deploy)

Ap√≥s o sucesso no upload, o desenvolvedor baixa o modelo para sua m√°quina local.

1.  Instalar [Ollama](https://ollama.com).
2.  **Execu√ß√£o via Link Direto (Hugging Face):**

```bash
ollama run hf.co/<SEU_USER>/planus-qwen-v1
```

### Customiza√ß√£o (Modelfile)

Para travar o Prompt do Sistema, crie um arquivo `Modelfile`:

```dockerfile
FROM ./planus.gguf
SYSTEM "Voc√™ √© o Tech Lead da Planuze, especialista em..."
PARAMETER temperature 0.3
```

E crie o modelo: `ollama create planus -f Modelfile`

---

## üö® Troubleshooting (Resumo de Erros Reais)

| Erro Observado                                  | Causa Raiz                                     | Solu√ß√£o Definitiva                                                              |
| :---------------------------------------------- | :--------------------------------------------- | :------------------------------------------------------------------------------ |
| `RuntimeError: No disk space left`              | Merge do modelo estourou o disco do Colab.     | Fazer upload direto (`push_to_hub_gguf`) ou limpar cache do Hugging Face antes. |
| `RuntimeError: llama.cpp folder does not exist` | Unsloth n√£o achou a pasta ou bin√°rio.          | Clonar manualmente e criar link simb√≥lico para o bin√°rio (passo da Fase 3).     |
| `make: ... Build system changed`                | O repo llama.cpp mudou de Make para CMake.     | Usar script de build com `cmake` (Fase 3).                                      |
| `c++: fatal error: Killed signal`               | Compila√ß√£o usou muita RAM (multithread).       | Compilar com flag `-j 1` (single thread).                                       |
| `HTTPError 401 Unauthorized`                    | Token inv√°lido, Read-only ou n√£o carregado.    | Gerar token **WRITE** no HF e confirmar carregamento do `.env`.                 |
| `HTTPError 403 Forbidden`                       | Tentativa de criar repo em org errada.         | Usar `api.whoami()['name']` para pegar o namespace correto.                     |
| `FileNotFoundError` (Config json)               | Drive demorou a sincronizar ou caminho errado. | Usar script de busca (`find`) ou `force_remount=True`.                          |
