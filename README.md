# ğŸ§  Planuze LLM Engine

Este repositÃ³rio contÃ©m um pipeline completo para **Fine-Tuning de Modelos de Linguagem (LLMs)** utilizando a biblioteca **Unsloth**. O projeto foi estruturado para facilitar o carregamento de modelos quantizados (4-bit), aplicaÃ§Ã£o de adaptadores LoRA, treinamento supervisionado (SFT) e exportaÃ§Ã£o para o formato GGUF.

## ğŸš€ Funcionalidades

- **Carregamento Otimizado**: Suporte a modelos 4-bit via Unsloth (ex: Llama-3, Qwen-2.5).
- **Fine-Tuning Eficiente**: Uso de LoRA/QLoRA para adaptaÃ§Ã£o de modelos grandes com menos memÃ³ria.
- **Pipeline de Dados**: Processamento automÃ¡tico de datasets no formato JSONL com templates de chat (formato Alpaca).
- **ExportaÃ§Ã£o GGUF**: ConversÃ£o automÃ¡tica do modelo treinado para GGUF, pronto para uso em ferramentas como Ollama, llama.cpp ou LM Studio.
- **ConfiguraÃ§Ã£o Modular**: SeparaÃ§Ã£o clara entre configuraÃ§Ãµes de modelo, treino e projeto.

## ğŸ“‚ Estrutura do Projeto

```text
planuze-llm/
â”œâ”€â”€ config/              # DefiniÃ§Ãµes de configuraÃ§Ã£o (Dataclasses)
â”œâ”€â”€ data/                # DiretÃ³rio para datasets (raw/train_data.jsonl)
â”œâ”€â”€ outputs/             # Checkpoints de treinamento (gerado automaticamente)
â”œâ”€â”€ src/                 # CÃ³digo fonte principal
â”‚   â”œâ”€â”€ data_handler.py  # Carregamento e formataÃ§Ã£o de dados
â”‚   â”œâ”€â”€ model_loader.py  # Gerenciamento do modelo e adapters
â”‚   â”œâ”€â”€ prompt_templates.py # Templates de prompt (Alpaca)
â”‚   â””â”€â”€ trainer_engine.py   # ConfiguraÃ§Ã£o do SFTTrainer
â”œâ”€â”€ main.py              # Ponto de entrada (Entrypoint)
â”œâ”€â”€ requirements.txt     # DependÃªncias do projeto
â””â”€â”€ trial.json           # Arquivo de exemplo (se aplicÃ¡vel ao formato)
```

## ğŸ› ï¸ PrÃ©-requisitos

- **Python** 3.10 ou superior.
- **GPU NVIDIA** (Recomendado para treino): Drivers CUDA instalados.
  - _Nota_: O cÃ³digo Ã© compatÃ­vel com desenvolvimento em Mac/CPU (apenas para estruturaÃ§Ã£o), mas o treinamento efetivo requer GPU compatÃ­vel com CUDA se usar as features do Unsloth.

## ğŸ“¦ InstalaÃ§Ã£o

1. **Clone o repositÃ³rio:**

   ```bash
   git clone <URL_DO_REPOSITORIO>
   cd planuze-llm
   ```

2. **Crie um ambiente virtual:**

   ```bash
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   # ou
   venv\Scripts\activate     # Windows
   ```

3. **Instale as dependÃªncias:**

   âš ï¸ **AtenÃ§Ã£o:** Verifique o arquivo `requirements.txt`.
   - Se estiver em **Linux com GPU**, descomente as linhas referentes ao `unsloth`, `xformers` e `trl`.
   - Se estiver em **MacOS** (sem GPU NVIDIA), mantenha as linhas do Unsloth comentadas.

   ```bash
   pip install -r requirements.txt
   ```

## âš™ï¸ ConfiguraÃ§Ã£o

A configuraÃ§Ã£o principal reside no arquivo `main.py` e `config/settings.py`.

No `main.py`, vocÃª ajusta o objeto `ProjectConfig`:

```python
project_config = ProjectConfig(
    model=ModelConfig(
        model_name="unsloth/Qwen2.5-32B-Instruct", # Modelo base
        max_seq_length=2048,
        load_in_4bit=True
    ),
    training=TrainingConfig(
        max_steps=60,         # Passos de treino
        batch_size=2,         # Tamanho do batch
        output_dir="outputs_checkpoints"
    ),
    dataset_path="data/raw/train_data.jsonl", # Caminho do dataset
    final_model_name="models/planus_qwen_v1"  # Caminho de saÃ­da do GGUF
)
```

## ğŸ“Š Formato dos Dados

O script espera um arquivo **JSONL** (JSON Lines) localizado em `data/raw/train_data.jsonl` (ou conforme configurado).

Cada linha deve conter um objeto JSON com os campos:

- `instruction`: A instruÃ§Ã£o do usuÃ¡rio.
- `input`: Contexto adicional.
- `output`: A resposta esperada.

**Exemplo:**

```json
{"instruction": "Resuma o texto.", "input": "O texto longo aqui...", "output": "Resumo aqui."}
{"instruction": "Classifique o sentimento.", "input": "Eu adorei este produto!", "output": "Positivo"}
```

O template utilizado (definido em `src/prompt_templates.py`) segue o padrÃ£o **Alpaca**.

## â–¶ï¸ Como Usar

Com tudo configurado e dependÃªncias instaladas, execute o pipeline:

```bash
python main.py
```

O script irÃ¡:

1. Carregar o modelo base.
2. Aplicar os adaptadores LoRA.
3. Carregar e formatar o dataset.
4. Executar o treinamento.
5. Salvar o modelo final em formato GGUF na pasta especificada.

## â“ SoluÃ§Ã£o de Problemas

- **FileNotFoundError**: Certifique-se de criar a pasta `data/raw` e adicionar o arquivo `train_data.jsonl`.
- **Erro de MemÃ³ria/CUDA**: Reduza o `batch_size` no `training` config ou use um modelo menor.

## ğŸ“„ LicenÃ§a

Este projeto Ã© de uso privado.
