# ğŸ§  LLM Fine-Tuner & Synthetic Data Pipeline

Este projeto fornece uma pipeline completa e agnÃ³stica para criar **Assistentes de IA Especializados** a partir de documentos brutos.

A arquitetura foi desenhada para ser utilizada em **qualquer domÃ­nio de conhecimento** (JurÃ­dico, MÃ©dico, TÃ©cnico, Educacional, etc.).

A pipeline automatiza trÃªs fases crÃ­ticas:

1.  **IngestÃ£o de Conhecimento:** ExtraÃ§Ã£o de texto de manuais, PDFs e TXTs.
2.  **GeraÃ§Ã£o de Dados SintÃ©ticos:** Uso de LLMs locais (via Ollama) para criar datasets de treino de alta qualidade (Perguntas & Respostas).
3.  **Fine-Tuning Eficiente:** Treinamento de modelos estado-da-arte (Llama 3, Qwen 2, Mistral) usando tÃ©cnicas de QLoRA/Unsloth.
4.  **ExportaÃ§Ã£o:** ConversÃ£o automÃ¡tica para GGUF para execuÃ§Ã£o local leve.

---

## ï¿½ Funcionalidades

- **Gerador de Dados SintÃ©ticos:** Transforma docs estÃ¡ticos em pares de instruÃ§Ã£o/resposta usando modelos como Llama 3 ou Qwen via Ollama.
- **Treinamento Otimizado (Unsloth):** Suporte nativo ao Unsloth para treinos 2x mais rÃ¡pidos e com 60% menos uso de VRAM.
- **ConfiguraÃ§Ã£o Centralizada:** Todo o controle via `.env` sem necessidade de alterar cÃ³digo.
- **Suporte a Modelos Modernos:** CompatÃ­vel com Llama 3.1, Qwen 2.5, Mistral Nemo e Gemma 2.
- **ExportaÃ§Ã£o GGUF:** GeraÃ§Ã£o automÃ¡tica de modelos quantizados prontos para uso no Ollama/LM Studio.

---

## ğŸ› ï¸ 1. PrÃ©-requisitos

### Hardware

- **Para GeraÃ§Ã£o de Dados:** Qualquer CPU decente (Apple Silicon M1/M2/M3 Ã© excelente) com 16GB+ RAM.
- **Para Treinamento:** GPU NVIDIA com suporte a CUDA (mÃ­nimo 8GB VRAM para modelos 8B, ideal 24GB para modelos 32B+). Suporta WSL2 no Windows e Linux nativo.

### Software

- **Python 3.10+**
- **Ollama** (para geraÃ§Ã£o de dados sintÃ©ticos). [Instalar Ollama](https://ollama.com/)
- **Gestor de Pacotes:** `uv` (recomendado) ou `pip`.

### Contas

- **Hugging Face:** Token (Write) para baixar modelos base e (opcionalmente) subir seu modelo treinado.
- **WandB (Opcional):** Para monitorar mÃ©tricas de treino.

---

## âš™ï¸ 2. InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 2.1. ConfiguraÃ§Ã£o do Projeto

```bash
# 1. Clone o repositÃ³rio
git clone <URL_DO_REPOSITORIO> my-llm-project
cd my-llm-project

# 2. Crie e ative o ambiente virtual
python3.10 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou venv\Scripts\activate no Windows

# 3. Instale as dependÃªncias
make install
```

> **Nota para usuÃ¡rios Mac:** Se estiver usando apenas para gerar dados, edite o `requirements.txt` e comente as dependÃªncias exclusivas da NVIDIA (unsloth, triton, xformers) antes de instalar, para evitar erros.

### 2.2. VariÃ¡veis de Ambiente

O coraÃ§Ã£o da customizaÃ§Ã£o estÃ¡ no arquivo `.env`.

1.  Copie o exemplo:
    ```bash
    cp .env.example .env
    ```
2.  Edite o `.env` com suas configuraÃ§Ãµes:
    - **HF_TOKEN:** Seu token Hugging Face.
    - **MODEL_NAME:** Modelo base (ex: `unsloth/Qwen2.5-7B-Instruct`).
    - **SYNTHETIC_SYSTEM_INSTRUCTION:** O prompt que define a "persona" do seu assistente. **Ã‰ aqui que vocÃª define se ele Ã© um advogado, mÃ©dico, suporte tÃ©cnico, etc.**

---

## ğŸ“š 3. Pipeline de Dados (Fase 1)

Nesta etapa, vocÃª transforma seus documentos brutos em um dataset de treino. Isso pode ser feito num MacBook ou PC sem GPU potente.

### Passo A: Documentos Fonte

Coloque seus arquivos PDF, TXT ou MD na pasta:
ğŸ“‚ **`data/source_documents/`**

### Passo B: Gerar Dataset

Execute o comando:

```bash
make data
```

**O que acontece nos bastidores:**

1.  O script lÃª cada arquivo em `data/source_documents/`.
2.  Quebra o texto em "chunks" (pedaÃ§os) configurÃ¡veis.
3.  Envia cada chunk para o Ollama (usando o modelo definido em `GENERATOR_MODEL`) com um prompt especial para criar perguntas e respostas baseadas naquele texto.
4.  Salva tudo em `data/raw/train_data_synthetic.jsonl`.
5.  Opcionalmente, mescla com dados manuais (`data/raw/manual_rules.jsonl`) se vocÃª tiver exemplos "gold standard" feitos Ã  mÃ£o.
6.  Gera o dataset final: ğŸ“‚ **`data/processed/train_dataset_final.jsonl`**.

---

## ğŸ‹ï¸ 4. Treinamento / Fine-Tuning (Fase 2)

Nesta etapa Ã© necessÃ¡ria uma GPU NVIDIA. Se vocÃª gerou dados no Mac, mova a pasta do projeto (ou apenas `data/processed/`) para sua mÃ¡quina de treino (Linux/WSL).

### Passo Ãšnico: Treinar

```bash
make train
```

**O que acontece nos bastidores (`main.py`):**

1.  Carrega o modelo base (configurado no `.env`) em 4-bit (QLoRA).
2.  Configura os adaptadores LoRA (apenas uma fraÃ§Ã£o dos pesos Ã© treinada).
3.  Inicia o treino usando os hiperparÃ¢metros do `.env` (Learning Rate, Batch Size, etc.).
4.  Ao final, **funde** os adaptadores LoRA no modelo base.
5.  Converte o modelo resultante para o formato **GGUF** (quantizado q4_k_m, por padrÃ£o).
6.  Salva o resultado em `models/<SEU_NOME_DE_MODELO>/`.

---

## ğŸ’¬ 5. Executar e Testar

ApÃ³s o treino, vocÃª terÃ¡ um arquivo `.gguf`. VocÃª pode usÃ¡-lo imediatamente no Ollama.

1.  Garanta que o `Modelfile` na raiz do projeto aponte para o caminho correto do seu novo modelo GGUF.
2.  Execute:

```bash
make run
```

Isso criarÃ¡ o modelo no Ollama e iniciarÃ¡ um chat no terminal.

---

## ğŸ“‚ Estrutura do Projeto

```text
.
â”œâ”€â”€ config/                 # MÃ³dulos de configuraÃ§Ã£o (Pydantic models)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ source_documents/   # [ENTRADA] Seus PDFs/Textos originais
â”‚   â”œâ”€â”€ raw/                # Dados intermediÃ¡rios gerados
â”‚   â””â”€â”€ processed/          # [SAÃDA] Dataset JSONL final para treino
â”œâ”€â”€ models/                 # Onde os modelos .gguf e adaptadores serÃ£o salvos
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ planuze/            # CÃ³digo fonte principal (pode ser renomeado para seu projeto)
â”‚   â”‚   â”œâ”€â”€ utils/          # Loggers e utilitÃ¡rios
â”‚   â”‚   â”œâ”€â”€ synthetic_data_gen.py
â”‚   â”‚   â””â”€â”€ data_handler.py
â”œâ”€â”€ .env                    # ConfiguraÃ§Ãµes globais (Segredos, HiperparÃ¢metros)
â”œâ”€â”€ Makefile                # Atalhos para comandos comuns
â”œâ”€â”€ main.py                 # Script principal de treinamento
â””â”€â”€ requirements.txt        # DependÃªncias Python
```

---

## ğŸ”§ PersonalizaÃ§Ã£o AvanÃ§ada

### Alterando a "Persona"

Para mudar o comportamento do modelo (ex: de Suporte TÃ©cnico para Assistente JurÃ­dico), altere a variÃ¡vel `SYNTHETIC_SYSTEM_INSTRUCTION` no arquivo `.env`. Isso mudarÃ¡ como os dados sintÃ©ticos sÃ£o gerados e, consequentemente, como o modelo aprende a responder.

### Ajuste de HiperparÃ¢metros

Se tiver pouca VRAM (ex: 8GB), ajuste no `.env`:

- `TRAINING_BATCH_SIZE=1`
- `TRAINING_GRAD_ACCUMULATION=4`
- `MAX_SEQ_LENGTH=2048` (ou menor)

---

## ğŸ¤ ContribuiÃ§Ã£o

Sinta-se livre para abrir Issues e Pull Requests. Este projeto Ã© um template base para democratizar o fine-tuning de LLMs.

## ğŸ“„ LicenÃ§a

MIT
